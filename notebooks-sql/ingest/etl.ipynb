{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ETL Process Overview\n",
    "\n",
    "## What is ETL\n",
    "ETL stands for Extract, Transform, Load  and it is a fundamental concept in data enginering and bussiness intelligence. \n",
    "\n",
    "**Extract**\n",
    "- E\n",
    "- Synonymous with Data Ingestion\n",
    "- The process of reading the data from various sources, which may include databases, spreadsheets, applications, APIs, or flat files\n",
    "\n",
    "**Transform**\n",
    "- T\n",
    "- The transformation of extracted data into a format suitable for analysis\n",
    "- May include \n",
    "  - Cleaning data \n",
    "    - removing duplicates \n",
    "    - handling missing values\n",
    "    - standardizing text formats\n",
    "  - Enriching data \n",
    "    - merging data from multiple sources\n",
    "    - calculating new metrics\n",
    "  - Restructuring data \n",
    "    - Pivoting\n",
    "    - Summarizing\n",
    "    - Normalizing\n",
    "  \n",
    "**Load**\n",
    "- L\n",
    "- Loading of the transformed data into a final destination, typically a data warehouse, database, or a datalake where it can be accessed, queried, and analyzed by end-users or applications\n",
    "\n",
    "## Difference between ETL and ELT\n",
    "\n",
    "In some workflows, particularly those that use modern data warehousing solutions like BigQuery, the ETL process may be modified into an ELT, where raw data is loaded into the warehouse before transformation. This takes advantage of the powerful processing capabilities of these platforms to perform transformations on the data after it has been loaded.\n",
    "\n",
    "## ETL in the Wind Resource Assessment Project\n",
    "\n",
    "While it is possible to do the entirety of the wind resource assessment using spreadsheets like excel, this is inefficient and non-scalable. Spreadsheets have lmitations on large datasets, make automation more difficult, and struggle with more complex calculations.\n",
    "\n",
    "\n",
    "\n",
    "**Extract**\n",
    "- The raw data will be extracted from Excelspreadsheets where it has been intially collected. This data comprises various metrics pertinent to wind resources such as wind speed, direction and gust measurements\n",
    "\n",
    "**Transform**\n",
    "- Involves cleansing the data to ensure its quality for analayis. For the context of this project, rows with null values which correspond to missing sensor data, will be dropped. \n",
    "  - This is justified because such missing data could distort the analysis\n",
    "\n",
    "**Load**\n",
    "- The cleansed and transformed data will be loaded onto a more robust and scalable data management system. Depending on the chose workflow, this can be a traditional SQL database like PostgreSQL, or a cloud-based data warehouse, like big-query. Or even a pandas dataframe. \n",
    "\n",
    "\n",
    "### Pure Python and Pandas Workflow\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### Mixed SQL and Python Workflow\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "We have several solutions available to us, including spreadsheets, as discussed below:\n",
    "- Pure Spreadsheets\n",
    "- Pure Python and Pandas Workflow\n",
    "- Mixed SQL and Python Workflow\n",
    "- Cloud such as Google BigQuery\n",
    "\n",
    "For the last three options, we need to load data into dataframes/databases, and we need to do some cleaning for all of the options. \n",
    "\n",
    "We know from looking at the data that there are rows with null values which correspond to missing sensor data. For the purpose of this project, we decide to just drop these rows, and not include the missing data in our analysis\n",
    "\n",
    "### Pure Python and Pandas Workflow\n",
    "\n",
    "We can work with the excel file directly, using the `pd.read_excel()` function to create a pandas dataframe which we can do some cleaning with related functions. We can then use pandas methods that can filter and aggregate the raw data into new pandas dataframes. \n",
    "\n",
    "### Mixed SQL and Python Workflow\n",
    "\n",
    "We can load the csv into a sql table, then use sql queries to filter and aggregate data as views. This may be desirable as a showcase of skill. Querying a dataset with SQL is also relatively easy. \n",
    "\n",
    "We have several options here\n",
    "- We can clean the dataset using excel in accordance with the schema of the table it will be loaded to, export that to a csv, then upload that csv directly to the table in the database (ETL)\n",
    "- We can export the dataset into a csv, load it into a staging table with a similar schema to our final table, then use a SQL query to load the data in the final table that also does the cleaning/filtering/renaming of things (ELT)\n",
    "- We can load the excel into a pandas dataframe, do all the cleaning and processing, output a csv, then upload that directly to the final table in the database (ETL)\n",
    "\n",
    "### Cloud such as BigQuery\n",
    "\n",
    "**To follow**\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pure Pandas ETL\n",
    "\n",
    "Version 1\n",
    "- As taken from previous work\n",
    "- Checks for valid input path\n",
    "- Uses try-except blocks to only allow excel and csv files\n",
    "- Included cleaning step\n",
    "\n",
    "Version 2\n",
    "- Added docstring\n",
    "- Added engine\n",
    "- rewrote try-except blocks\n",
    "- included checking if file exists, valid file extension\n",
    "\n",
    "Version 2.5\n",
    "- changed some names\n",
    "- fixed docstring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Version 2.5\n",
    "import pandas as pd\n",
    "import os \n",
    "\n",
    "def pandas_pure_etl(input_path: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Perform an ETL process on an Excel or CSV data file using pandas with the in order columns of date-time, wind-speed, gust-speed, and wind-direction\n",
    "    \n",
    "    Extracts the data from the input_path provided, transforms it by removing null values and ensureing a datetime data type. \n",
    "    Loads the data into a pandas data frame\n",
    "\n",
    "    Parameters:\n",
    "        input_path (str): The file system path to the data file. The file must be either an Excel (.xlsx) or CSV (.csv) format.\n",
    "\n",
    "    Raises:\n",
    "        FileNotFoundError: If the file specified by 'input_path' does not exist.\n",
    "        ValueError: If the file is neither an Excel file nor a CSV file, or if it cannot be read.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A DataFrame containing cleaned wind data with the columns: 'date_time', 'wind_speed', 'gust_speed', and 'wind_direction'.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Check if file exists\n",
    "    if not os.path.isfile(input_path):\n",
    "        raise FileNotFoundError(f\"The file does not exist: {input_path}\")\n",
    "    \n",
    "    # Check if valid extension\n",
    "    file_extension = os.path.splitext(input_path)[1].lower()\n",
    "    if file_extension not in [\".xlsx\",\".csv\"]:\n",
    "        raise ValueError(\"File is not an Excel or CSV file.\")\n",
    "    \n",
    "    # Try to load dataframe according to extension\n",
    "    try:\n",
    "        if file_extension == \".xlsx\":  \n",
    "            df = pd.read_excel(\n",
    "                io=input_path,\n",
    "                engine=\"openpyxl\",\n",
    "                names=[\"date_time\",\"wind_speed\",\"gust_speed\",\"wind_direction\"]\n",
    "            )\n",
    "        else:\n",
    "            df = pd.read_csv(\n",
    "                io=input_path,\n",
    "                names=[\"date_time\",\"wind_speed\",\"gust_speed\",\"wind_direction\"]\n",
    "            )\n",
    "    except Exception as e:\n",
    "        raise ValueError(f\"Failed to read file: {e}\")\n",
    "        \n",
    "    # Remove rows with null values\n",
    "    # Ensure date_time is actually datetime\n",
    "    df = df.dropna().reset_index(drop=True)\n",
    "    df[\"date_time\"] = pd.to_datetime(df[\"date_time\"])\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mixed SQL ETL\n",
    "\n",
    "Made up of 2 functions \n",
    "\n",
    "- pandas_extract_transform()\n",
    "- sql_load()\n",
    "\n",
    "Version 2 ET\n",
    "- docstring\n",
    "- comments\n",
    "- check input file existence\n",
    "- try-except using read_excel\n",
    "- cleans null values\n",
    "- provides default output name if none given\n",
    "- \n",
    "\n",
    "Version 2.5\n",
    "- Fixed docstring\n",
    "- added file extension checking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Version 2.5\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "def pandas_extract_transform(input_path : str, output_name : str = None) -> str:\n",
    "    \"\"\"\n",
    "    Performs ET by extracting data from an excel file, cleaning data by removing rows with null values, and then writing to a csv.\n",
    "    If output_name is not provided, CSV file is named after the excel file and is generated in the same directory\n",
    "\n",
    "    Parameters:\n",
    "    - input_path (str): The file path to the source Excel file.\n",
    "    - output_name (str, optional): The desired name for the output CSV file.\n",
    "\n",
    "    Raises:\n",
    "    - FileNotFoundError: If the Excel file specified by 'input_path' does not exist.\n",
    "    - ValueError: If the Excel file cannot be read.\n",
    "        \n",
    "    Returns:\n",
    "    - str: The file path to the created CSV file.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Check if file exists\n",
    "    if not os.path.isfile(input_path):\n",
    "        raise FileNotFoundError(f\"The file does not exist: {input_path}\")\n",
    "    \n",
    "    # Check if valid extension\n",
    "    file_extension = os.path.splitext(input_path)[1].lower()\n",
    "    if file_extension != \".xlsx\":\n",
    "        raise ValueError(\"File is not an Excel file.\")\n",
    "    \n",
    "    # Try to load excel file\n",
    "    try:\n",
    "        df = pd.read_excel(\n",
    "            io=input_path,\n",
    "            engine=\"openpyxl\",\n",
    "            names=[\"date_time\",\"wind_speed\",\"gust_speed\",\"wind_direction\"]\n",
    "        )\n",
    "    except Exception as e:\n",
    "        raise ValueError(f\"Could not read excel file: {e}\")\n",
    "    \n",
    "    # Remove rows with null values\n",
    "    df = df.dropna().reset_index(drop=True)\n",
    "    \n",
    "    # Provide output_name if not given\n",
    "    if output_name is None:\n",
    "        base = os.path.splitext(input_path)[0]\n",
    "        output_name = f\"{base}.csv\"\n",
    "    \n",
    "    # Assigns output path to be the same directory    \n",
    "    output_path = os.path.join(os.path.dirname(input_path), output_name)\n",
    "    \n",
    "    if os.path.isfile(output_path):\n",
    "        overwrite = input(f\"File {output_name} already exists. Overwrite? y/n\").lower()\n",
    "        if overwrite != \"y\":\n",
    "            print(\"Operation cancelled by user\")\n",
    "            return None\n",
    "    \n",
    "    # Generates csv\n",
    "    df.to_csv(path_or_buf=output_path, index=False, encoding=\"utf-8\")\n",
    "    \n",
    "    return output_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "def pandas_extract_transform(input_path : str, output_name : str = None) -> str:\n",
    "    # Check that file exists\n",
    "    if not os.path.isfile(input_path):\n",
    "        raise FileNotFoundError(f\"The file does not exist: {input_path}\")\n",
    "    \n",
    "    # Check that file is excel format\n",
    "    file_extension = os.path.splitext(input_path)[1].lower()\n",
    "    if file_extension != \".xlsx\":\n",
    "        raise ValueError(\"File is not an Excel file.\")\n",
    "    \n",
    "    # Load excel into pandas dataframe. Re-names column headers\n",
    "    try:\n",
    "        df = pd.read_excel(\n",
    "            io=input_path,\n",
    "            engine=\"openpyxl\",\n",
    "            names=[\"date_time\",\"wind_speed\",\"gust_speed\",\"wind_direction\"]\n",
    "        )\n",
    "    except Exception as e:\n",
    "        raise ValueError(f\"Could not read excel file: {e}\")\n",
    "    \n",
    "    # Drop rows containing null values. \n",
    "    df = df.dropna().reset_index(drop=True)\n",
    "    \n",
    "    # Provide name of the result csv if none is given\n",
    "    if output_name is None:\n",
    "        base = os.path.splitext(input_path)[0]\n",
    "        output_name = f\"{base}.csv\"\n",
    "    \n",
    "    # Assigns output path to be the same directory    \n",
    "    output_path = os.path.join(os.path.dirname(input_path), output_name)\n",
    "    \n",
    "    if os.path.isfile(output_path):\n",
    "        overwrite = input(f\"File {output_name} already exists. Overwrite? y/n\").lower()\n",
    "        if overwrite != \"y\":\n",
    "            print(\"Operation cancelled by user\")\n",
    "            return None\n",
    "    \n",
    "    # Generates CSV file\n",
    "    df.to_csv(path_or_buf=output_path, index=False, encoding=\"utf-8\")\n",
    "    \n",
    "    return output_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/gioabeleda/Desktop/wind-energy-dashboard-streamlit/data/wind_energy.csv'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "csv = pandas_extract_transform(\"/Users/gioabeleda/Desktop/wind-energy-dashboard-streamlit/data/wind_energy.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/gioabeleda/Desktop/wind-energy-dashboard-streamlit/data/wind_energy.csv'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Version 1\n",
    "- first version that actually works lol\n",
    "- params moved to function instead of hard coded\n",
    "- docstring\n",
    "- file exist check\n",
    "- file extension check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Version 1\n",
    "# Expects a csv file with columns date-time, wind-speed, gust-speed, wind-direction\n",
    "# Expects already existing database, schema, and table\n",
    "\n",
    "import os\n",
    "import psycopg2\n",
    "\n",
    "def sql_load(input_path : str, dbname : str = \"wind_energy_psycopg\",\n",
    "             user: str = \"postgres\", password : str = \"postgres\", \n",
    "             host: str = \"localhost\", schema : str = \"wind_sites\", \n",
    "             table : str = \"upd_wind_site\") -> None:\n",
    "    \"\"\"\n",
    "    Loads data from a CSV file into a PostgreSQL table using psycopg2.\n",
    "\n",
    "    This function opens a CSV file from the specified path and loads its contents into the given PostgreSQL table. \n",
    "    The CSV file must have a header row with the fields corresponding to the database table columns. \n",
    "    The database connection is managed within the function, and it uses COPY command for efficient bulk data loading.\n",
    "\n",
    "    Parameters:\n",
    "    - input_path (str): Absolute path to the CSV file to be loaded.\n",
    "    - dbname (str): Name of the database to connect to. Default is \"wind_energy_psycopg\".\n",
    "    - user (str): Username for authentication. Default is \"postgres\".\n",
    "    - password (str): Password for authentication. Default is \"postgres\".\n",
    "    - host (str): Host address of the database. Default is \"localhost\".\n",
    "    - schema (str): Schema name of the target table. Default is \"wind_sites\".\n",
    "    - table (str): Table name where data will be loaded. Default is \"upd_wind_site\".\n",
    "\n",
    "    Returns:\n",
    "    - None\n",
    "\n",
    "    Raises:\n",
    "    - FileNotFoundError: If the CSV file does not exist at the specified path.\n",
    "    - ValueError: If the specified file is not a CSV file.\n",
    "    - psycopg2.DatabaseError: If an error occurs during the database operation.\n",
    "\n",
    "    Note:\n",
    "    - The function will commit the transaction if the COPY command is successful, or rollback the transaction if an exception occurs.\n",
    "    - Ensure that the PostgreSQL user has the required permissions to perform a COPY operation on the specified table.\n",
    "    - The function assumes that the CSV file is formatted correctly with the necessary headers and delimiters.\n",
    "    \"\"\"\n",
    "\n",
    "    # Check if file exists\n",
    "    if not os.path.isfile(input_path):\n",
    "        raise FileNotFoundError(f\"The file does not exist: {input_path}\")\n",
    "    \n",
    "    # Check if valid file extension\n",
    "    file_extension = os.path.splitext(input_path)[1].lower()\n",
    "    if file_extension != \".csv\":\n",
    "        raise ValueError(\"File is not a csv file\")\n",
    "    \n",
    "    parameters = {\n",
    "        \"dbname\" : dbname,\n",
    "        \"user\" : user,\n",
    "        \"password\" : password,\n",
    "        \"host\" : host\n",
    "    }\n",
    "    \n",
    "    # pscyopg2 connection block that does the actual loading process\n",
    "    with psycopg2.connect(**parameters) as connection:\n",
    "        with connection.cursor() as cursor:\n",
    "            with open(input_path, \"r\") as csv:\n",
    "                try: \n",
    "                    cursor.copy_expert(\n",
    "                        f\"\"\"\n",
    "                        COPY {schema}.{table} (date_time, wind_speed, gust_speed, wind_direction)\n",
    "                        FROM STDIN\n",
    "                        DELIMITER ','\n",
    "                        CSV HEADER\n",
    "                        \"\"\",\n",
    "                        csv)\n",
    "                \n",
    "                except Exception as e:\n",
    "                    connection.rollback()\n",
    "                    raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql_load(\"/Users/gioabeleda/Desktop/wind-energy-dashboard-streamlit/data/wind_energy.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Excel Conversion No Cleaning\n",
    "Version 1\n",
    "- Docstring\n",
    "- try-except to check excel file\n",
    "- Renaming columns \n",
    "\n",
    "Version 2\n",
    "- Check if file exists (FileNotFoundError)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset is converted to CSV in Excel or pandas\n",
    "# Creates temporary staging table \n",
    "\n",
    "import psycopg2\n",
    "\n",
    "def sql_elt_staging(csv_path : str = None, \n",
    "                    dbname : str = \"wind_energy_psycopg\",\n",
    "                    temp_table : str = \"wind_site_raw\",\n",
    "                    final_table : str = \"upd_wind_site\", \n",
    "                    ) -> None: \n",
    "\n",
    "    if csv_path is None:\n",
    "        raise ValueError(\"No valid file path\")\n",
    "    \n",
    "    parameters = {\n",
    "        \"dbname\" : dbname,\n",
    "        \"user\" : \"postgres\",\n",
    "        \"password\" : \"postgres\",\n",
    "        \"host\" : \"localhost\"\n",
    "    }\n",
    "    \n",
    "    with psycopg2.connect(**parameters) as connection:\n",
    "        with connection.cursor() as cursor:\n",
    "            \n",
    "            # Create a staging table\n",
    "            cursor.execute(\n",
    "                f\"\"\"\n",
    "                CREATE TEMP TABLE {temp_table} (\n",
    "                    LIKE wind_sites.{final_table}\n",
    "                    EXCLUDING CONSTRAINTS\n",
    "                );\n",
    "                \"\"\"\n",
    "            )\n",
    "            \n",
    "            # Insert data from csv into staging table\n",
    "            cursor.execute(\n",
    "                f\"\"\"\n",
    "                COPY {temp_table} \n",
    "                FROM {csv_path}  \n",
    "                DELIMITER ','\n",
    "                CSV HEADER;\n",
    "                \"\"\"\n",
    "            )\n",
    "            \n",
    "            # Using a filter, copy only valid data to the final table\n",
    "            cursor.execute(\n",
    "                f\"\"\"\n",
    "                INSERT INTO {final_table}\n",
    "                \"\"\"\n",
    "            )\n",
    "            \n",
    "            # Drop the staging table\n",
    "            cursor.execute(\n",
    "                f\"\"\"\n",
    "                DROP TABLE {temp_table};\n",
    "                \"\"\"\n",
    "            )\n",
    "            \n",
    "            connection.commit()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "virtual_environment",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
